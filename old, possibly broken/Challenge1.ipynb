{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This challenge serves two purposes:\n",
    "1. introduce you to the world of Policy Gradient methods,\n",
    "2. act as an evaluation for the RL class.\n",
    "\n",
    "It is meant to be started in class and finished at home. It will require that you read quite a bit, then that you work on your own understanding, before answering the questions below.\n",
    "\n",
    "The goal is simply to complete your discovery of Reinforcement Learning and to insure that you validate the skill-goals of the class. No traps here, I'd be happy to give a perfect mark to anybody that completes this exam and even better for those who go beyond (see the bonus questions for that). \n",
    "\n",
    "I recommend to answer both questions of the theoretical part first, then move on to the implementation part, then get back to the second question of the theoretical part (practice allows your ideas to mature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Policy gradient theorem (4 points)\n",
    "\n",
    "Use your favourite source of information to:\n",
    "1. quote the policy gradient theorem (2 points)\n",
    "2. explain how it's useful (2 points)\n",
    "\n",
    "The goal is not to have you search dark places of the web for references, but to build an understanding for yourself and render it in your own words. You can, for instance, use the following sources (you may suggest other ones):\n",
    "- [Policy gradient algorithms](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html) on Lilian Weng's blog (OpenAI)\n",
    "- [Policy gradient methods for reinforcement learning with function approximation](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf), Sutton, McAllester, Sigh, Mansour. NIPS 2000.\n",
    "- [Policy gradient methods](http://www.scholarpedia.org/article/Policy_gradient_methods) on Scholarpedia (written by Jan Peters).\n",
    "- [Reinforcement Learning, an introduction](http://incompleteideas.net/book/the-book.html), the classic book by Sutton and Barto. Chapter 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The policy gradient theorem:**\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The meaning and usefulness of the policy gradient theorem:**\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE (3 points)\n",
    "\n",
    "- Implement the REINFORCE algorithm (from the Machine Learning journal paper \"Simple statistical gradient-following algorithms for connectionist reinforcement learning\" by Williams 1992, but also explained in all links above) on OpenAI gym's inverted pendulum.\n",
    "- Plot the evolution of performance vs training time\n",
    "- Discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "#import gym.envs.box2d.lunar_lander as ll\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendulum = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic (3 points)\n",
    "\n",
    "- Implement an Actor-Critic algorithm on OpenAI gym's inverted pendulum.\n",
    "- Plot the evolution of performance vs training time\n",
    "- Discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search (5 points)\n",
    "\n",
    "First note that the trick below allows you to set the pendulum state as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendulum.reset()\n",
    "pendulum.unwrapped.state = [np.pi/3., 0.] # format: theta, thetaDot\n",
    "pendulum.render();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that the integration time step is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time step:\", pendulum.unwrapped.dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that you can use the trick below to control the wall clock execution time of a method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "time_limit = datetime.timedelta(seconds=pendulum.unwrapped.dt)\n",
    "count = 0\n",
    "begin = datetime.datetime.utcnow()\n",
    "while datetime.datetime.utcnow() - begin < time_limit:\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use all those to implement a Monte Carlo Tree Search method that controls the pendulum in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus questions (extra points)\n",
    "\n",
    "That part is free, I'm just providing hints.\n",
    "\n",
    "- Take a look at the Deterministic Policy Gradient Theorem ([Deterministic policy gradient algorithms](http://proceedings.mlr.press/v32/silver14.html) by Silver et al, 2014) and the DDPG algorithm ([Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971), Lillicrap et al. 2015), discuss, implement, etc.\n",
    "- Pick any more recent algorithm from [Lilian Weng's excellent summary](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html) and implement it in a demonstrative manner.\n",
    "- Take a more difficult environment and try to solve it (for instance acrobot, cart-pole or mountain-car).\n",
    "- Take a question that seems difficult for you or in the litterature and illustrate why, try to answer it, etc.\n",
    "- Get inspiration from these papers from friends [CEM-RL](https://arxiv.org/abs/1810.01222), [overview on policy search](https://arxiv.org/abs/1803.04706), [GEP-PG](https://arxiv.org/abs/1802.05054)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
