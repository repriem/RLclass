{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice session**\n",
    "\n",
    "Build your own RL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T13:47:30.371607Z",
     "start_time": "2019-01-16T13:47:24.049382Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "#import gym.envs.box2d.lunar_lander as ll\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three environments in this practical session.\n",
    "\n",
    "- [Blackjack](https://gym.openai.com/envs/Blackjack-v0/). The classic casino game.\n",
    "- [Inverted Pendulum](https://gym.openai.com/envs/Pendulum-v0/). The inverted pendulum swingup problem is a classic problem in the control literature. In this version of the problem, the pendulum starts in a random position, and the goal is to swing it up so it stays upright.\n",
    "- [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/). Land a lunar module on the moon (just like the Chinese recently did).\n",
    "\n",
    "<div class=\"alert alert-warning\"><b> Exercice:</b><br>\n",
    "    Use the previous classes and the \"getting started\" below to implement an agent that learns an optimal control policy for one or several environments. Start with blackjack for a toy problem then move on to one of the two other ones.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T13:47:32.827340Z",
     "start_time": "2019-01-16T13:47:30.374722Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emmanuel/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "blackjack = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T13:47:32.839063Z",
     "start_time": "2019-01-16T13:47:32.830054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BlackjackEnv in module gym.envs.toy_text.blackjack object:\n",
      "\n",
      "class BlackjackEnv(gym.core.Env)\n",
      " |  Simple blackjack environment\n",
      " |  \n",
      " |  Blackjack is a card game where the goal is to obtain cards that sum to as\n",
      " |  near as possible to 21 without going over.  They're playing against a fixed\n",
      " |  dealer.\n",
      " |  Face cards (Jack, Queen, King) have point value 10.\n",
      " |  Aces can either count as 11 or 1, and it's called 'usable' at 11.\n",
      " |  This game is placed with an infinite deck (or with replacement).\n",
      " |  The game starts with each (player and dealer) having one face up and one\n",
      " |  face down card.\n",
      " |  \n",
      " |  The player can request additional cards (hit=1) until they decide to stop\n",
      " |  (stick=0) or exceed 21 (bust).\n",
      " |  \n",
      " |  After the player sticks, the dealer reveals their facedown card, and draws\n",
      " |  until their sum is 17 or greater.  If the dealer goes bust the player wins.\n",
      " |  \n",
      " |  If neither player nor dealer busts, the outcome (win, lose, draw) is\n",
      " |  decided by whose sum is closer to 21.  The reward for winning is +1,\n",
      " |  drawing is 0, and losing is -1.\n",
      " |  \n",
      " |  The observation of a 3-tuple of: the players current sum,\n",
      " |  the dealer's one showing card (1-10 where 1 is ace),\n",
      " |  and whether or not the player holds a usable ace (0 or 1).\n",
      " |  \n",
      " |  This environment corresponds to the version of the blackjack problem\n",
      " |  described in Example 5.1 in Reinforcement Learning: An Introduction\n",
      " |  by Sutton and Barto (1998).\n",
      " |  http://incompleteideas.net/sutton/book/the-book.html\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BlackjackEnv\n",
      " |      gym.core.Env\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, natural=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  reset(self)\n",
      " |      Resets the state of the environment and returns an initial observation.\n",
      " |      \n",
      " |      Returns: observation (object): the initial observation of the\n",
      " |          space.\n",
      " |  \n",
      " |  seed(self, seed=None)\n",
      " |      Sets the seed for this env's random number generator(s).\n",
      " |      \n",
      " |      Note:\n",
      " |          Some environments use multiple pseudorandom number generators.\n",
      " |          We want to capture all such seeds used in order to ensure that\n",
      " |          there aren't accidental correlations between multiple generators.\n",
      " |      \n",
      " |      Returns:\n",
      " |          list<bigint>: Returns the list of seeds used in this env's random\n",
      " |            number generators. The first value in the list should be the\n",
      " |            \"main\" seed, or the value which a reproducer should pass to\n",
      " |            'seed'. Often, the main seed equals the provided 'seed', but\n",
      " |            this won't be true if seed=None, for example.\n",
      " |  \n",
      " |  step(self, action)\n",
      " |      Run one timestep of the environment's dynamics. When end of\n",
      " |      episode is reached, you are responsible for calling `reset()`\n",
      " |      to reset this environment's state.\n",
      " |      \n",
      " |      Accepts an action and returns a tuple (observation, reward, done, info).\n",
      " |      \n",
      " |      Args:\n",
      " |          action (object): an action provided by the environment\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): agent's observation of the current environment\n",
      " |          reward (float) : amount of reward returned after previous action\n",
      " |          done (boolean): whether the episode has ended, in which case further step() calls will return undefined results\n",
      " |          info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.core.Env:\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  close(self)\n",
      " |      Override _close in your subclass to perform any necessary cleanup.\n",
      " |      \n",
      " |      Environments will automatically close() themselves when\n",
      " |      garbage collected or when the program exits.\n",
      " |  \n",
      " |  render(self, mode='human')\n",
      " |      Renders the environment.\n",
      " |      \n",
      " |      The set of supported modes varies per environment. (And some\n",
      " |      environments do not support rendering at all.) By convention,\n",
      " |      if mode is:\n",
      " |      \n",
      " |      - human: render to the current display or terminal and\n",
      " |        return nothing. Usually for human consumption.\n",
      " |      - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
      " |        representing RGB values for an x-by-y pixel image, suitable\n",
      " |        for turning into a video.\n",
      " |      - ansi: Return a string (str) or StringIO.StringIO containing a\n",
      " |        terminal-style text representation. The text can include newlines\n",
      " |        and ANSI escape sequences (e.g. for colors).\n",
      " |      \n",
      " |      Note:\n",
      " |          Make sure that your class's metadata 'render.modes' key includes\n",
      " |            the list of supported modes. It's recommended to call super()\n",
      " |            in implementations to use the functionality of this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (str): the mode to render with\n",
      " |          close (bool): close all open renderings\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      class MyEnv(Env):\n",
      " |          metadata = {'render.modes': ['human', 'rgb_array']}\n",
      " |      \n",
      " |          def render(self, mode='human'):\n",
      " |              if mode == 'rgb_array':\n",
      " |                  return np.array(...) # return RGB frame suitable for video\n",
      " |              elif mode is 'human':\n",
      " |                  ... # pop up a window and render\n",
      " |              else:\n",
      " |                  super(MyEnv, self).render(mode=mode) # just raise an exception\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gym.core.Env:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  unwrapped\n",
      " |      Completely unwrap this env.\n",
      " |      \n",
      " |      Returns:\n",
      " |          gym.Env: The base non-wrapped gym.Env instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from gym.core.Env:\n",
      " |  \n",
      " |  action_space = None\n",
      " |  \n",
      " |  metadata = {'render.modes': []}\n",
      " |  \n",
      " |  observation_space = None\n",
      " |  \n",
      " |  reward_range = (-inf, inf)\n",
      " |  \n",
      " |  spec = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(blackjack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:37:24.968683Z",
     "start_time": "2019-01-16T10:37:24.964547Z"
    }
   },
   "outputs": [],
   "source": [
    "# state = [player's current sum, dealer's one showing card, player has a usable ace]\n",
    "x = blackjack.reset()\n",
    "print(x)\n",
    "# actions = 0: stick (stop their turn), 1: hit (ask for an additional card)\n",
    "print(\"action space: \", blackjack.action_space)\n",
    "# rewards: -1 for loosing, +1 for winning\n",
    "y,r,d,_ = blackjack.step(0)\n",
    "print(y)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:13:37.630672Z",
     "start_time": "2019-01-16T10:13:37.622834Z"
    }
   },
   "outputs": [],
   "source": [
    "pendulum = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:13:38.220320Z",
     "start_time": "2019-01-16T10:13:38.214764Z"
    }
   },
   "outputs": [],
   "source": [
    "# State space, action space\n",
    "# x = (cos(theta), sin(theta), thetaDot)\n",
    "x = pendulum.reset()  # random initialization\n",
    "print(x)\n",
    "# action = applied torque\n",
    "a = [0.]\n",
    "print(\"action space type:\", pendulum.action_space)\n",
    "print(\"lower bound on torque:\", pendulum.action_space.low)\n",
    "print(\"upper bound on torque:\", pendulum.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:13:42.212063Z",
     "start_time": "2019-01-16T10:13:40.288958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transitions, rendering\n",
    "pendulum.render()\n",
    "for i in range(100):\n",
    "    y,r,d,_=pendulum.step(a)\n",
    "    pendulum.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this session, you will work with discrete action spaces so, even though the simulator takes continous actionsas we only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:13:47.962086Z",
     "start_time": "2019-01-16T10:13:46.327904Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simplified, discrete actions space\n",
    "actions = [[-2.], [0.], [2.]]\n",
    "for i in range(100):\n",
    "    y,r,d,_=pendulum.step(actions[2])\n",
    "    pendulum.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:13:51.027720Z",
     "start_time": "2019-01-16T10:13:51.005084Z"
    }
   },
   "outputs": [],
   "source": [
    "lunarLander = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:15:51.009649Z",
     "start_time": "2019-01-16T10:15:51.005126Z"
    }
   },
   "outputs": [],
   "source": [
    "# State space, action space\n",
    "# x = [position_x, \n",
    "#      position_y, \n",
    "#      velocity_x, \n",
    "#      velocity_y, \n",
    "#      angle, \n",
    "#      angular_velocity, \n",
    "#      left_leg_touches_ground, \n",
    "#      right_leg_touches_ground]\n",
    "# action 0: do nothing, \n",
    "# action 1: fire left orientation engine, \n",
    "# action 2: fire main engine,\n",
    "# action 3: fire right orientation engine\n",
    "x = lunarLander.reset()\n",
    "print(x)\n",
    "print(lunarLander.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:15:53.432449Z",
     "start_time": "2019-01-16T10:15:51.780722Z"
    }
   },
   "outputs": [],
   "source": [
    "lunarLander.render()\n",
    "for i in range(100):\n",
    "    y,r,d,_=lunarLander.step(np.random.randint(4))\n",
    "    lunarLander.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:16:54.943593Z",
     "start_time": "2019-01-16T10:16:54.932742Z"
    }
   },
   "outputs": [],
   "source": [
    "pendulum.close()\n",
    "lunarLander.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your RL agent\n",
    "\n",
    "Use the above examples, the previous classes and your imagination to experiment with your own RL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
