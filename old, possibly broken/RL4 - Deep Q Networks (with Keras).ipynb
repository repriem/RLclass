{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class 4: Deep Q Networks.**\n",
    "\n",
    "1. [Everything you need to know](#everything)\n",
    "2. [Deep Q-networks](#dqn)\n",
    "    1. [Pong](#breakout)\n",
    "    2. [State space](#state)\n",
    "    3. [Action space](#action)\n",
    "    4. [Reminder on Q-learning](#reminder)\n",
    "    5. [Q-networks](#qn)\n",
    "    6. [Deep Q-networks](#dqnetworks)\n",
    "    7. [Experience replay](#xpr)\n",
    "3. [Going further in Deep RL](#moreDeepRL)\n",
    "4. [Deep Tic-Tac-Toe learning](#ttt)\n",
    "    1. [The board game](#board)\n",
    "    2. [Deep Q-Learning](#ttt-dqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"everything\"></a>Everything you need to know\n",
    "\n",
    "Everything you should remember after this session.<br>\n",
    "<br>\n",
    "<div class=\"alert alert-success\">\n",
    "<ul>\n",
    "<li> Deep Q-Learning is Q-learning with a Deep Neural Network as function approximator for $Q$.\n",
    "<li> Deep Q-Network (DQN): take $s$ as input and output a Q-value per action.\n",
    "<li> State space for visual tasks (like Atari game): stack the last $m$ frames in the state, $s = (im_0,\\ldots,im_m)$.\n",
    "<li> Experience replay: store a memory of the $N$ last samples and pick a minibatch of $n$ samples for the DQN parameters update at each time step.\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Of course, all this seems very obscure right now and the block above will only serve as a reminder when you re-open the notebook later. We will introduce every concept intuitively and progessively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"dqn\"></a>Deep Q-networks\n",
    "\n",
    "The ideas we present in this notebook were introduced in the crucial paper **[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)** by Mnih et al. (2013) and was later popularized by DeepMind's paper in Nature **[Human-level control through deep reinforcement learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)** by Mnih et al. (2015).\n",
    "\n",
    "The core idea we develop here is to use a Deep Neural Network architecture to learn the optimal Q-function in a Q-learning algorithm. We will follow closely the presentation of the first reference above (8 pages which are definitely worth reading) and replicate their approach.\n",
    "\n",
    "Nota: the second paper above actually introduce some improvements and notable differences with the first reference. We review them at the end of this section.\n",
    "\n",
    "## <a id=\"breakout\"></a>Pong\n",
    "\n",
    "Let's start by taking a challenging problem: we want to build an agent that learns to play Pong, one of the [Atari games](https://github.com/openai/gym/blob/master/gym/envs/atari/atari_env.py) in Gym (originally in the [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment)).\n",
    "\n",
    "You might want to try different games later on (like the popular Breakout game for instance or some games from the [classic control](https://github.com/openai/gym/tree/master/gym/envs/classic_control) series of environments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.envs.atari as atari\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the environment's description.\n",
    "> Maximize your score in the Atari 2600 game Pong. In this environment, the observation is an RGB image of the screen, which is an array of shape (210, 160, 3). Each action is repeatedly performed for a duration of k frames, where k is uniformly sampled from $\\{2, 3, 4\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.observation_space.shape)\n",
    "print(np.min(env.observation_space.low))\n",
    "print(np.max(env.observation_space.high))\n",
    "print(env.action_space)\n",
    "#help(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.reset()\n",
    "plt.imshow(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"state\"></a>State space\n",
    "\n",
    "Let's take a look at the characteristics of this game. Breakout is one of the classic Atari 2600 games. We want to build a \"human-level\" controler so we will define the state of our agent by the set of raw pixels from the current image. This might not be enough to define an informative enough state: for instance, having just a snapshot image only gives us the position of the ball but not its velocity. So we will expand the state so that it contains the 4 last frames from the game.\n",
    "\n",
    "One frame is a $210\\times 160$ RGB image with a 256 color palette, so the set of all possible frames has size $256^{210 \\times 160 \\times 3} \\sim 10^{242579}$. That's a little too many for an efficient enumeration. Even by converting the image to greyscale, downsampling to a $110\\times 84$ and then cropping to a $84\\times 84$ image to keep only the playing area (as we shall do a little later to slightly simplify the problem), that's still around $10^{16980}$ possible stages. So, definitely, this discrete problem is not suitable for complete enumeration.\n",
    "\n",
    "Of course, most of the possible images will never occur in a Breakout game and the true state space is actually a much smaller subset of the full set of possible images. Nevertheless, unless we provide a large engineering effort in describing the state space with few variables (which would be contradictory of our goal of a \"human-level\" AI) we will need to automatically discover some structure in the state sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def process_screen(x):\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_RGB2GRAY)\n",
    "    return cv2.resize(x, (84, 110), interpolation=cv2.INTER_AREA)[17:101,:]\n",
    "\n",
    "\n",
    "env = gym.make('Pong-v4')\n",
    "x = env.reset()\n",
    "plt.imshow(x)\n",
    "print(x.shape, x.dtype)\n",
    "y = process_screen(x)\n",
    "plt.figure()\n",
    "plt.imshow(y, cmap=\"gray\")\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the 4 last frames\n",
    "z = np.stack([y,y,y,y],axis=-1)\n",
    "print(z.shape, z.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"action\"></a>Action space\n",
    "\n",
    "There are 18 buttons on the Atari controller. However not all games use all buttons. Our interface to Pong specifies 6 possible actions:\n",
    "- 0 NOOP (no operation)\n",
    "- 1 FIRE (press fire button, doesn't do anything in Pong)\n",
    "- 2 RIGHT (actually moves the paddle up in Pong)\n",
    "- 3 LEFT (actually moves the paddle left in Pong)\n",
    "- 4 UP (moves the paddle upwards)\n",
    "- 5 DOWN (moves the paddle downwards)\n",
    "\n",
    "But really, it goes up to the 6th action for naming consistency (UP and DOWN), because the other actions are not really useful.\n",
    "\n",
    "Also, for an unknown reason, the game does not start until the 20th frame (but always starts automatically, pressing FIRE does not change anything).\n",
    "\n",
    "The frame rate is 60Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(atari.atari_env.ACTION_MEANING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid confusion between the 6 actions allowed by Gym, let's build a wrapper around our environment, with only 2 possible actions (\"0\" for UP and \"1\" for DOWN).\n",
    "\n",
    "TODO expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import AtariPreprocessing\n",
    "\n",
    "class PongWrapper(AtariPreprocessing):\n",
    "    def __init__(self, env, **kwargs):\n",
    "        super(PongWrapper, self).__init__(env, **kwargs)\n",
    "    def step(self,action):\n",
    "        return super(PongWrapper, self).step(4+action)\n",
    "    def _get_obs(self):\n",
    "        if self.frame_skip > 1:  # more efficient in-place pooling\n",
    "            np.maximum(self.obs_buffer[0], self.obs_buffer[1], out=self.obs_buffer[0])\n",
    "        obs = cv2.resize(self.obs_buffer[0], (84, 110), interpolation=cv2.INTER_AREA)[17:101,:]\n",
    "\n",
    "        if self.scale_obs:\n",
    "            obs = np.asarray(obs, dtype=np.float32) / 255.0\n",
    "        else:\n",
    "            obs = np.asarray(obs, dtype=np.uint8)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PongWrapper(gym.make('PongNoFrameskip-v4'),\n",
    "                  noop_max=0,\n",
    "                  frame_skip=4,\n",
    "                  terminal_on_life_loss=True,\n",
    "                  grayscale_obs=True,\n",
    "                  scale_obs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trying a random agent in Pong\n",
    "import time\n",
    "\n",
    "env.reset()\n",
    "env.render()\n",
    "for i in range(20):\n",
    "    a = np.random.randint(2)\n",
    "    x,_,_,_=env.step(a)\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "env.close()\n",
    "print(\"shape: \", x.shape, \", min = \", x.min(), \", max = \", x.max(), \", dtype = \", x.dtype, sep='')\n",
    "plt.imshow(x, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"reminder\"></a>Reminder on Q-learning\n",
    "\n",
    "Let's take a minute on the white board for a general recap on everything we have introduced in past classes, to get to this point.\n",
    "\n",
    "<blockquote> The idea that we learn by interacting with our environment is probably the first to occur to us when we think about the nature of learning. When an infant plays, waves its arms, or looks about, it has no explicit teacher, but it does have a direct sensorimotor connection to its environment. Exercising this connection produces a wealth of information about cause and effect, about the consequences of actions, and about what to do in order to achieve goals. Throughout our lives, such interactions are undoubtedly a major source of knowledge about our environment and ourselves. Whether we are learning to drive a car or to hold a conversation, we are acutely aware of how our environment responds to what we do, and we seek to influence what happens through our behavior. Learning from interaction is a foundational idea underlying nearly all theories of learning and intelligence. (Richard S. Sutton)</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"qn\"></a>Q-networks\n",
    "\n",
    "Recall Q-learning with function approximation:\n",
    "<blockquote>\n",
    "Repeat:\n",
    "<ol>\n",
    "<li> In $s$, choose $a$ (*GLIE actor*)\n",
    "<li> Observe $r$, $s'$\n",
    "<li> Temporal difference: $\\delta=r+\\gamma \\max_{a'} Q_\\theta(s',a') - Q_\\theta(s,a)$\n",
    "<li> Update $Q_\\theta$: $\\theta \\leftarrow \\theta + \\alpha \\delta \\nabla_\\theta Q_\\theta(s,a)$\n",
    "<li> $s\\leftarrow s'$\n",
    "</ol>\n",
    "</blockquote>\n",
    "\n",
    "Let's re-build this result while considering a neural network function approximator $Q_\\theta(s,a)$ with weights $\\theta$. We will refer to a neural network function approximator as a **Q-network**.\n",
    "\n",
    "<img src=\"images/dqlas.png\" height=\"15%\" width=\"15%\"></img>\n",
    "\n",
    "Given a set of weights $\\theta$ defining a given Q-network, one can define a target $y$ as:\n",
    "$$y(s,a) = \\mathbb{E}_{s'}\\left( r + \\gamma \\max_{a'} Q_{\\theta}(s',a') \\Big| s,a\\right)$$\n",
    "\n",
    "That $y$ target is the expected (over all states $s'$ reachable from $(s,a)$) 1-step best value, given that $Q_\\theta$ will be obtained in $s'$. In a Value Iteration fashion, we would like a new Q-network to fit this target, and thus to minimize the loss:\n",
    "$$L(\\theta') = \\mathbb{E}_{s,a} \\left[ \\left(y(s,a) - Q_{\\theta'}(s,a)\\right)^2 \\right]$$\n",
    "\n",
    "That loss is the mean squared error of the Q-network (over all states and actions).\n",
    "\n",
    "Our goal is to find the Q-network that fits $Q^*$. To obtain this network, we define a **sequence** of loss functions $L_i(\\theta)$ that change at each iteration $i$:\n",
    "$$L_i(\\theta) = \\mathbb{E}_{s,a} \\left[ \\left(y_i(s,a) - Q_\\theta(s,a)\\right)^2 \\right]$$\n",
    "\n",
    "It is important to note several things:\n",
    "- $L_i(\\theta)$ depends on $y_i(s,a) = \\mathbb{E}_{s'}\\left( r + \\gamma \\max_{a'} Q_{\\theta_{i-1}}(s',a') \\Big| s,a\\right)$ and thus on $\\theta_{i-1}$.\n",
    "- $\\theta_{i-1}$ is held fixed when optimizing $\\theta_i$.\n",
    "\n",
    "Differentiating the loss function with respect to the weights $\\theta$ we obtain:\n",
    "$$\\nabla_\\theta L_i(\\theta) = -2 \\mathbb{E}_{s,a,s'} \\left[ \\left( r + \\gamma \\max_{a'} Q_{\\theta_{i-1}} (s',a') - Q_\\theta(s,a) \\right) \\nabla_\\theta Q_\\theta(s,a) \\right]$$\n",
    "\n",
    "Let's take a minute to consider the $\\mathbb{E}_{s,a,s'}$ in this gradient. The probability of observing a pair $s,a$ is called the *behaviour distribution*; when sampling by interacting with the environment, it depends on the policy being applied. But in our case, we want to cover the full reachable state-action space efficiently. That is, we want to have non-zero probability in all reachable state action pairs and (preferably) important probability mass on pairs that are important for the optimal policy. This is, in the end, a reformulation of the exploration vs. exploitation compromise, which can be solved using $\\epsilon$-greedy sampling strategies. The probability of observing $s'$ can be estimated by the sampling by interaction, it requires however to obtain enough samples in each pair $(s,a)$.\n",
    "\n",
    "Rather than computing the full expectation in the gradient above, one often turns to stochastic gradient descent to minimize $L_i$. If the weights are updated every time a sample $(s,a,r,s')$ is collected and if the expectation $\\mathbb{E}_{s,a,s'}$ is replaced by single sample estimates from a behaviour distribution and the environment's dynamics, then one obtains the standard Q-learning update given at the beginning of this section.\n",
    "$$\\delta=r+\\gamma \\max_{a'} Q_{\\theta_{i-1}}(s',a') - Q_\\theta(s,a)$$\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\delta \\nabla_\\theta Q_\\theta(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"dqnetworks\"></a>Deep Q-networks\n",
    "\n",
    "Let's define a Convolutional Neural Network (CNN) that will predict our Q-values. When we choose a greedy action, we need to evaluate $Q(s,a)$ for all possible $a$. So in our case, that means propagating $(s,a)$ values 2 times through our network. In a more complex game that requires $n$ buttons on the Atari controller, that would require $n$ propagations through the network. It would be more efficient to build a network that predicts the values of the 2 actions in a single pass for a given state $s$. So we define the input of our Q-network as the state only, and the output as the 2-dimensional vector evaluating each action.\n",
    "\n",
    "<img src=\"images/dqls.png\" height=\"30%\" width=\"30%\"></img>\n",
    "\n",
    "Then, our network has the following structure:\n",
    "- input: $84\\times 84\\times 4$ image (the last 4 frames)\n",
    "- layer 1: Convolutions with 16 filters of size $8\\times 8$ and stride 4. The activation is a ReLU function.\n",
    "- layer 2: Convolutions with 32 filters of size $4\\times 4$ and stride 2. The activation is a ReLU function.\n",
    "- layer 3: Fully connected with 256 ReLU units\n",
    "- layer 4 (output): Fully connected with 2 linear units (one for each action's value)\n",
    "\n",
    "Graphically, this yields the following network structure.\n",
    "<img src=\"images/dqn.png\"></img>\n",
    "<img src=\"images/dqn_keras.png\"></img>\n",
    "\n",
    "We refer to this type of CNN as *Deep Q-Networks*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "dqn = Sequential()\n",
    "#1st layer\n",
    "dqn.add(Conv2D(filters=16, kernel_size=(8,8), strides=4, activation=\"relu\", input_shape=(84,84,4)))\n",
    "#2nd layer\n",
    "dqn.add(Conv2D(filters=32, kernel_size=(4,4), strides=2, activation=\"relu\"))\n",
    "dqn.add(Flatten())\n",
    "#3rd layer\n",
    "dqn.add(Dense(units=256, activation=\"relu\"))\n",
    "#output layer\n",
    "dqn.add(Dense(units=2, activation=\"linear\"))\n",
    "\n",
    "dqn.compile(optimizer=\"rmsprop\", loss=\"mean_squared_error\")\n",
    "\n",
    "#from keras.utils import plot_model\n",
    "#plot_model(dqn, to_file=\"images/dqn_keras.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"xpr\"></a>Experience replay\n",
    "\n",
    "It is known that learning directly from consecutive samples is inefficient. The main reason is that these samples are strongly correlated with each other and learning sample by sample has the same risk of *catastrophic forgetting* that we already saw in Deep Learning. In other words, we risk forgetting about what was learned previously in some region of the state-action space, if we flood the Q-network with too many samples from another region.\n",
    "\n",
    "In Deep Learning, this limitation was overcome by the introduction of mini-batches. Experience replay in Reinforcement Learning is a very similar approach that stores samples $(s,a,r,s')$ in a so-called *replay memory* $\\mathcal{D}$ for $N$ time steps. Then, when updating the network's parameters, a mini-batch of $\\mathcal{D}$ is uniformly sampled in order to compute the gradient.\n",
    "\n",
    "Experience replay has three main advantages:\n",
    "- Each sample $(s,a,r,s')$ can be reused in many updates (although this is not a N-step update, it allows a better usage of samples).\n",
    "- Randomizing the mini-batch over the replay memory breaks the correlations between samples and thus reduces the variance of weights updates.\n",
    "- The bias in exploration due to overestimating some Q-values (because of a tendency of neural networks to focus on the last samples) is reduced. Thus exploration is more efficient and we have less risk of getting stuck in local minima of the loss function.\n",
    "\n",
    "<a href=\"#xp-replay\" data-toggle=\"collapse\"> Remarks</a><br>\n",
    "<div id=\"xp-replay\" class=\"collapse\">\n",
    "<ul>\n",
    "<li> Our replay memory has finite length, we can still forget old samples. We have made our algorithm more robust but, yes, we still face catastrophic forgetting. For more refined approaches at remembering long term experience, see the usage of Recurrent Neural Networks in <a href=\"https://arxiv.org/pdf/1507.06527.pdf\">this paper</a> for instance.\n",
    "<li> The choice of uniform sampling within the replay memory is somehow a bit naive. We have seen in previous classes that the ordering of Bellman backups (in the model-based case) or TD updates (in the model-free case) have an importance in the convergence speed of the algorithm. Therefore, it would be beneficial to use a non-uniform distribution over samples in our experience replay procedure. This was developped in the <a href=\"https://arxiv.org/pdf/1511.05952.pdf\">Prioritized Experience Replay</a> algorithm. That uses the TD error to approximate the Bellman residual and define update priorities.\n",
    "<li> Experience replay *requires* an off-policy algorithm (thus it is not compatible with SARSA).\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Given this last idea, lets implement an Experience Replay Q-learning using our DQN. The algorithm's pseudo-code is:\n",
    "\n",
    "         state = init()\n",
    "         loop:\n",
    "            action = greedy_action(DQN) or random_action()\n",
    "            new_state, reward = step(state, action)\n",
    "            replay_memory.add(state, action, reward, new_state)\n",
    "            minibatch = replay_memory.sample(minibatch_size)\n",
    "            X_train = Y_train = []\n",
    "            for (s,a,r,s') in minibatch:\n",
    "                Q  = DQN.predict(s)\n",
    "                Q' = DQN.predict(s')\n",
    "                if non-terminal(s'): \n",
    "                    update = r + gamma * max(Q')    \n",
    "                else:  \n",
    "                    update = r\n",
    "                Q[a] = update\n",
    "                X_train.add(s)\n",
    "                Y_train.add(Q)\n",
    "            DQN.train_one_step(X_train,Y_train)\n",
    "            state = new_state\n",
    "\n",
    "A few algorithmic details:\n",
    "- Total number of steps: $10^7$.\n",
    "- Replay memory size: $10^6$ samples.\n",
    "- Mini-batch size: $32$ samples.\n",
    "- $\\gamma = 0.99$\n",
    "- Reward clipping. To control the scale of the global scores, clip the rewards so that all strictly positive rewards are $+1$, all zero rewards remain at $0$, and strictly negative rewards become $-1$.\n",
    "- Epsilon greedy. Decrease linearly $\\epsilon$ from $1$ to $0.1$ over the first million steps and keep it at $0.1$ thereafter.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "**Warning:** the network training defined below might take very (very!) long. For example, on an i7 CPU with no GPU it takes about 10 days. Also, the replay memory of $10^7$ samples uses around 10Gb of RAM as defined below. If your machine's resources are below that, you might want to scale down the replay memory size. Therefore, only run this code if you have time ahead of you. A simpler example is given later in the notebook.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = 10000000\n",
    "replay_memory_size = 1000000\n",
    "mini_batch_size = 32\n",
    "gamma = 0.99\n",
    "\n",
    "def epsilon(step):\n",
    "    if step<1e6:\n",
    "        return 1.-step*9e-7\n",
    "    return .1\n",
    "\n",
    "def clip_reward(r):\n",
    "    rr=0\n",
    "    if r>0:\n",
    "        rr=1\n",
    "    if r<0:\n",
    "        rr=-1\n",
    "    return rr\n",
    "\n",
    "def greedy_action(network, x):\n",
    "    Q = network.predict(np.array([x]))\n",
    "    return np.argmax(Q)\n",
    "\n",
    "def MCeval(network, trials, length, gamma):\n",
    "    print(\"\\n\")\n",
    "    scores = np.zeros((trials))\n",
    "    for i in range(trials):\n",
    "        print('\\r', 'trial', i)\n",
    "        screen_x = env.reset()\n",
    "        stacked_x = deque([screen_x, screen_x, screen_x, screen_x], maxlen=4)\n",
    "        x = np.stack(stacked_x, axis=-1)\n",
    "        for t in range(length):\n",
    "            #print(\"\\n\")\n",
    "            #print('\\r', \"step\", t)\n",
    "            a = greedy_action(network, x)\n",
    "            screen_y, r, d, _ = env.step(a)\n",
    "            r = clip_reward(r)\n",
    "            scores[i] = scores[i] + gamma**t * r\n",
    "            if d:\n",
    "                # restart episode\n",
    "                break\n",
    "            else:\n",
    "                # keep going\n",
    "                screen_x = screen_y\n",
    "                stacked_x.append(screen_x)\n",
    "                x = np.stack(stacked_x, axis=-1)\n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class for the replay memory\n",
    "from collections import deque\n",
    "\n",
    "class MemoryBuffer:\n",
    "    \"An experience replay buffer using numpy arrays\"\n",
    "    def __init__(self, length, screen_shape, action_shape):\n",
    "        self.length = length\n",
    "        self.screen_shape = screen_shape\n",
    "        self.action_shape = action_shape\n",
    "        shape = (length,) + screen_shape\n",
    "        self.screens_x = np.zeros(shape, dtype=np.uint8) # starting states\n",
    "        self.screens_y = np.zeros(shape, dtype=np.uint8) # resulting states\n",
    "        shape = (length,) + action_shape\n",
    "        self.actions = np.zeros(shape, dtype=np.uint8) # actions\n",
    "        self.rewards = np.zeros((length,1), dtype=np.uint8) # rewards\n",
    "        self.terminals = np.zeros((length,1), dtype=np.bool) # true if resulting state is terminal\n",
    "        self.terminals[-1] = True\n",
    "        self.index = 0 # points one position past the last inserted element\n",
    "        self.size = 0 # current size of the buffer\n",
    "    \n",
    "    def append(self, screenx, a, r, screeny, d):\n",
    "        self.screens_x[self.index] = 256*screenx\n",
    "        #plt.imshow(screenx)\n",
    "        #plt.show()\n",
    "        #plt.imshow(self.screens_x[self.index])\n",
    "        #plt.show()\n",
    "        self.actions[self.index] = a\n",
    "        self.rewards[self.index] = r\n",
    "        self.screens_y[self.index] = 256*screeny\n",
    "        self.terminals[self.index] = d\n",
    "        self.index = (self.index+1) % self.length\n",
    "        self.size = np.min([self.size+1,self.length])\n",
    "    \n",
    "    def stacked_normalized_frames(self, index, im_source):\n",
    "        im_deque = deque(maxlen=4)\n",
    "        pos = index % self.length\n",
    "        for i in range(4): \n",
    "            im = im_source[pos]\n",
    "            im_deque.appendleft(im)\n",
    "            test_pos = (pos-1) % self.length\n",
    "            if self.terminals[test_pos] == False:\n",
    "                pos = test_pos\n",
    "        return np.stack(im_deque, axis=-1)/256\n",
    "    \n",
    "    def stacked_normalized_frames_x(self, index):\n",
    "        return self.stacked_normalized_frames(index, self.screens_x)\n",
    "    \n",
    "    def stacked_normalized_frames_y(self, index):\n",
    "        return self.stacked_normalized_frames(index, self.screens_y)\n",
    "    \n",
    "    def minibatch(self, mb_size):\n",
    "        #return np.random.choice(self.data[:self.size], size=sz, replace=False)\n",
    "        indices = np.random.choice(self.size, size=mb_size, replace=False)\n",
    "        x = np.zeros((mb_size,)+self.screen_shape+(4,))\n",
    "        y = np.zeros((mb_size,)+self.screen_shape+(4,))\n",
    "        for i in range(mb_size):\n",
    "            x[i] = self.stacked_normalized_frames_x(indices[i])\n",
    "            y[i] = self.stacked_normalized_frames_y(indices[i])\n",
    "        return x, self.actions[indices], self.rewards[indices], y, self.terminals[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize state and replay memory\n",
    "screen_x = env.reset()\n",
    "stacked_x = deque([screen_x, screen_x, screen_x, screen_x], maxlen=4)\n",
    "x = np.stack(stacked_x, axis=-1)\n",
    "replay_memory = MemoryBuffer(replay_memory_size, (84, 84), (1,))\n",
    "# initial state for evaluation\n",
    "evaluation_period = 100\n",
    "Xtest = np.array([x])\n",
    "nb_epochs = total_steps // evaluation_period\n",
    "epoch=-1\n",
    "scoreQ = np.zeros((nb_epochs))\n",
    "scoreMC = np.zeros((nb_epochs))\n",
    "\n",
    "# Deep Q-learning with experience replay\n",
    "for step in range(total_steps):\n",
    "    print(\"\\r\", \"step =\", step, \"/\", total_steps, end=\"\", flush=True)\n",
    "    # evaluation\n",
    "    if(step%evaluation_period == 0):\n",
    "        #print(\"\\r\", \"step =\", step, \"/\", total_steps, end=\"\")\n",
    "        epoch = epoch+1\n",
    "        plt.plot(scoreQ[0:epoch])\n",
    "        # evaluation of initial state\n",
    "        scoreQ[epoch] = np.mean(dqn.predict(Xtest).max(axis=1))\n",
    "        # roll-out evaluation\n",
    "        #scoreMC[epoch] = MCeval(network=dqn, trials=20, length=700, gamma=gamma)\n",
    "    # action selection\n",
    "    if np.random.rand() < epsilon(step):\n",
    "        a = np.random.randint(2)\n",
    "    else:\n",
    "        a = greedy_action(dqn, x)\n",
    "    # step\n",
    "    screen_y, r, d, _ = env.step(a)\n",
    "    r = clip_reward(r)\n",
    "    replay_memory.append(screen_x, a, r, screen_y, d)\n",
    "    # train\n",
    "    if step>mini_batch_size:\n",
    "        X,A,R,Y,D = replay_memory.minibatch(mini_batch_size)\n",
    "        QY = dqn.predict(Y)\n",
    "        QYmax = QY.max(1).reshape((mini_batch_size,1))\n",
    "        update = R + gamma * (1-D) * QYmax\n",
    "        QX = dqn.predict(X)\n",
    "        QX[np.arange(mini_batch_size), A.ravel()] = update.ravel()\n",
    "        dqn.train_on_batch(x=X, y=QX)\n",
    "    # prepare next transition\n",
    "    if d==True:\n",
    "        # restart episode\n",
    "        screen_x = env.reset()\n",
    "        stacked_x = deque([screen_x, screen_x, screen_x, screen_x], maxlen=4)\n",
    "        x = np.stack(stacked_x, axis=-1)\n",
    "    else:\n",
    "        # keep going\n",
    "        screen_x = screen_y\n",
    "        stacked_x.append(screen_x)\n",
    "        x = np.stack(stacked_x, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox\n",
    "screen_x = env.reset()\n",
    "stacked_x = deque([screen_x, screen_x, screen_x, screen_x], maxlen=4)\n",
    "x = np.stack(stacked_x, axis=-1)\n",
    "Xtest = np.array([x])\n",
    "for i in range(700):\n",
    "    #dqn.predict(Xtest)\n",
    "    env.step(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a video of what you should get (along and) after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"p88R2_3yWPA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The video below shows the evolution of training on Breakout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"TmPfTpjtdgg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice**:<br>\n",
    "Read the [Nature paper](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning) on DQN and note the differences with the implementation above. Then modify the code above to match the contents of this paper.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two key differences are:\n",
    "- target network\n",
    "- bigger network size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"moreDeepRL\"></a>Going further in Deep RL\n",
    "\n",
    "Lots of things to learn, the field has been blooming for the last years. Parallelization, sample efficiency, distribution estimation, network stabilization... many hot topics.\n",
    "\n",
    "In ISAE: SuReLI, the Supaero Reinforcement Learning Initiative!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <a id=\"ttt\"></a>Deep Tic-Tac-Toe learning\n",
    "\n",
    "Let's implement the DQN algorithm on a simple Tic-Tac-Toe game.\n",
    "\n",
    "## <a id=\"board\"></a>The board game\n",
    "\n",
    "This is a quite simple implementation. The board is a tuple of size 9 where each action refers to a position in the tuple. We store the status of the current player and who won the game.\n",
    "\n",
    "We see above the function that we have to develop ... Lets do that.\n",
    "\n",
    "The main classes and objects:\n",
    "\n",
    "* joueur(state) \n",
    "* available_move(state)\n",
    "* next_state(state, action, current_player)\n",
    "* win(board, player)\n",
    "* payoff(current_player, state) \n",
    "* play(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oxo:\n",
    "    def __init__(self):\n",
    "        self.current_player = 1\n",
    "        self.actions = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "        self.board = (0,0,0,0,0,0,0,0,0)\n",
    "        self.nb_move = 0\n",
    "        self.end_game = 0 # -1: null, O: running, 1: player 1 win, 2: player 2 win\n",
    "\n",
    "    def play(self, state, action):\n",
    "        self.current_player = self.joueur(state)\n",
    "        stateList = list(self.board)\n",
    "        stateList[action] = self.current_player\n",
    "        self.board = tuple(stateList)\n",
    "        self.nb_move += 1\n",
    "        #print(\"nb move: \", self.nb_move)\n",
    "        self.actions.remove(action)\n",
    "        \n",
    "        if self.asWin():\n",
    "            self.end_game = self.current_player\n",
    "        if self.nb_move == 9 and self.end_game == 0:\n",
    "            self.end_game = -1\n",
    "\n",
    "    def next_state(self, state, action, player):\n",
    "        stateList = list(state)\n",
    "        stateList[action] = player\n",
    "        return tuple(stateList)\n",
    "\n",
    "    def available_move(self, state):\n",
    "        am = []\n",
    "        i = 0;\n",
    "        for x in state:\n",
    "            if x == 0: \n",
    "                am += [i]\n",
    "            i += 1\n",
    "        return am\n",
    "\n",
    "    def asWin(self):\n",
    "        p = self.current_player\n",
    "        b = self.board\n",
    "        if (b[0] == b[1] == b[2] == p or b[3] == b[4] == b[5] == p or b[6] == b[7] == b[8] == p or\n",
    "            b[0] == b[3] == b[6] == p or b[1] == b[4] == b[7] == p or b[2] == b[5] == b[8] == p or\n",
    "            b[0] == b[4] == b[8] == p or b[2] == b[4] == b[6] == p):\n",
    "            return True\n",
    "        else: return False\n",
    "\n",
    "    def win(self, b, p):\n",
    "        if (b[0] == b[1] == b[2] == p or b[3] == b[4] == b[5] == p or b[6] == b[7] == b[8] == p or\n",
    "            b[0] == b[3] == b[6] == p or b[1] == b[4] == b[7] == p or b[2] == b[5] == b[8] == p or\n",
    "            b[0] == b[4] == b[8] == p or b[2] == b[4] == b[6] == p):\n",
    "            return True\n",
    "        else: return False\n",
    "        \n",
    "\n",
    "    # -1: runnin, 0; execo, 1 joueur 1, 2 joueur 2    \n",
    "    def payoff(self, p, b):\n",
    "        nb_move = 0\n",
    "        for i in b:\n",
    "            if i != 0: nb_move += 1\n",
    "\n",
    "        if self.win(b, 1):    return 1\n",
    "        if self.win(b, 2):    return 2    \n",
    "        if nb_move == 9: return 0        \n",
    "        return -1\n",
    "\n",
    "    def joueur(self, board):\n",
    "        J1=0\n",
    "        J2=0\n",
    "        for i in board:\n",
    "            if i==1: J1+=1 \n",
    "            if i==2: J2+=1 \n",
    "        if J1==J2: return 1\n",
    "        return(2)\n",
    "\n",
    "    def simulation(self):\n",
    "        while self.end_game == 0:\n",
    "            self.myPrint()\n",
    "            print (\"actions \", self.actions)\n",
    "            action = int(input(\"Player %s: \" % (self.current_player)))\n",
    "\n",
    "            if action in self.actions: self.play(self.board, action)\n",
    "            else: print (\"wrong move, try again\")\n",
    "\n",
    "            if self.asWin(): \n",
    "                print(\"Player \" , self.current_player , \" win!!!\")\n",
    "                self.end_game = self.current_player\n",
    "\n",
    "            if self.nb_move == 9 and self.end_game == 0: \n",
    "                print(\"No winner, No looser!\")\n",
    "                self.end_game == -1\n",
    "\n",
    "            if self.current_player == 1: self.current_player = 2\n",
    "            else: self.current_player = 1\n",
    "\n",
    "    def myPrint(self):\n",
    "        b = []\n",
    "        for x in self.board:\n",
    "            if x == 1: b.append('X')\n",
    "            else: \n",
    "                if x == 2: b.append('O')\n",
    "                else: b.append('.')\n",
    "        print()\n",
    "        print(\"     \", b[0] , \"  \" , b[1] , \"  \" , b[2], \"       \", 0 , \"  \" , 1 , \"  \" , 2)\n",
    "        print(\"     \", b[3] , \"  \" , b[4] , \"  \" , b[5], \"  ->   \", 3 , \"  \" , 4 , \"  \" , 5)\n",
    "        print(\"     \", b[6] , \"  \" , b[7] , \"  \" , b[8], \"       \", 6 , \"  \" , 7 , \"  \" , 8)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"ttt-dqn\"></a>Deep Q learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop, sgd\n",
    "from keras.layers.recurrent import LSTM\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to build our deep neural network model. \n",
    "\n",
    ">Be aware that there is no formal way to design deep neural networks and the accuracy of your neural architectures will depends on your expererience, knowledge and ... intuition :)\n",
    "\n",
    "Our architecture should take in input a state vector. According to our board game implementation, a state is a vector of 9 digits: (0,0,0,0,0,0,0,0,0) that correspond to the 9 squares of our game board. In output our architecture should provide 9 values that correspond to the Q-value for the 9 possible actions.\n",
    "\n",
    "I suggest two very basic architectures, but feel free to try your own. \n",
    "\n",
    "Note: Dense layer takes in input a 2D vector. LTSM takes a 3D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "'''\n",
    "# !! input shape = (1, 1, len(input)) !!\n",
    "model.add(LSTM(input_dim=9,output_dim=150,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(150,return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(9, init='lecun_uniform'))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "'''\n",
    "\n",
    "model.add(Dense(150, init='lecun_uniform', input_shape=(9,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Dense(150, init='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(9, init='lecun_uniform'))\n",
    "model.add(Activation('linear'))\n",
    "model.compile(loss='mse', optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some parameters used by our deep network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "gamma = 0.99 # discount factor\n",
    "epsilon = 1 # epsilon-greddy\n",
    "\n",
    "update=0 \n",
    "alpha=0.1 # learning rate\n",
    "experience_replay=True\n",
    "batchSize = 40 # mini batch size\n",
    "buffer = 1000\n",
    "replay = [] # init vector buffer\n",
    "h=0 # current size of the vector buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeu = oxo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and develop our deep Q learning with and without minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    jeu = oxo()\n",
    "    state = jeu.board\n",
    "    current_player = jeu.joueur(state)\n",
    "    \n",
    "    while(jeu.end_game==0):\n",
    "        current_player = jeu.joueur(state)\n",
    "        qval = model.predict(np.array(state).reshape(1,len(state)), batch_size=batchSize)\n",
    "        if (random.random() < epsilon): # exploration exploitation strategy    \n",
    "            rand = np.random.randint(0,len(jeu.actions))\n",
    "            action = jeu.actions[rand]\n",
    "        else: #choose best action from Q(s,a) values\n",
    "            qval_av_action = [-9999]*9\n",
    "            for ac in jeu.actions:\n",
    "                qval_av_action[ac] = qval[0][ac]\n",
    "            action = (np.argmax(qval_av_action))\n",
    "        #Take action, observe new state S'\n",
    "        #Observe reward\n",
    "        jeu.play(state, action)\n",
    "        new_state = jeu.board\n",
    "        # choose new reward values\n",
    "        reward = -5\n",
    "        if jeu.payoff(current_player, new_state) == current_player:\n",
    "            reward = 2\n",
    "        if jeu.payoff(current_player, new_state) == 0:\n",
    "            reward = 1\n",
    "        if jeu.payoff(current_player, new_state) == -1:\n",
    "            reward = 0\n",
    "            \n",
    "        if not experience_replay:\n",
    "            #Get max_Q(S',a)\n",
    "            newQ = model.predict(np.array(new_state).reshape(1,len(state)), batch_size=batchSize)\n",
    "            maxQ = np.max(newQ)\n",
    "            y = np.zeros((1,9))\n",
    "            y[:] = qval[:]\n",
    "            if reward != 0: #non-terminal state\n",
    "                update = (reward + gamma * maxQ)\n",
    "            else:\n",
    "                update = reward\n",
    "            y[0][action] = update #target output\n",
    "            print(\"Game #: %s\" % (i,))\n",
    "            model.fit(np.array(state).reshape(1, len(state)), y, batch_size=batchSize, nb_epoch=1, verbose=1)\n",
    "            state = new_state\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        else:\n",
    "            #Experience replay storage\n",
    "            if (len(replay) < buffer): #if buffer not filled, add to it\n",
    "                replay.append((state, action, reward, new_state))\n",
    "            else: #if buffer full, overwrite old values\n",
    "                if (h < (buffer-1)):\n",
    "                    h += 1\n",
    "                else:\n",
    "                    h = 0\n",
    "                replay[h] = (state, action, reward, new_state)\n",
    "                #randomly sample our experience replay memory\n",
    "            \n",
    "            if(len(replay)>batchSize):\n",
    "                minibatch = random.sample(replay, batchSize)\n",
    "                X_train = []\n",
    "                y_train = []\n",
    "                for memory in minibatch:\n",
    "                    #Get max_Q(S',a)\n",
    "                    old_state, action, reward, new_state = memory\n",
    "                    old_qval = model.predict(np.array(old_state).reshape(1,len(old_state)), batch_size=1)\n",
    "                    newQ = model.predict(np.array(new_state).reshape(1,len(new_state)), batch_size=1)\n",
    "                    maxQ = np.max(newQ)\n",
    "                    y = old_qval[:]\n",
    "                    if reward != 0: #non-terminal state\n",
    "                        update = (reward + (gamma * maxQ))\n",
    "                    else: #terminal state\n",
    "                        update = reward\n",
    "                    y[0][action] = update\n",
    "                    X_train.append(np.array(old_state).reshape(len(old_state),))\n",
    "                    y_train.append(np.array(y).reshape(9,))\n",
    "    \n",
    "                X_train = np.array(X_train)\n",
    "                y_train = np.array(y_train)\n",
    "                print(\"Game #: %s\" % (i,))\n",
    "                model.fit(X_train, y_train, batch_size=batchSize, nb_epoch=1, verbose=1)\n",
    "                state = new_state\n",
    "            clear_output(wait=True)\n",
    "        # update exploitation / exploration strategy\n",
    "        if epsilon > 0.1:\n",
    "            epsilon -= (1.0/epochs)\n",
    "    \n",
    "        # save the model every 1000 epochs\n",
    "        if i%1000 == 0:\n",
    "            model.save(\"model_dql_oxo_dense.dqf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, we could develop a test function to play with our deep Q learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAlgo(model, playerIA):\n",
    "    loop = True\n",
    "    while (loop):\n",
    "        jeu = oxo()\n",
    "        #switch first player at each tour\n",
    "        if playerIA==1:\n",
    "            playerIA=2\n",
    "        else:\n",
    "            playerIA=1\n",
    "        #while game still in progress\n",
    "        while(jeu.end_game==0):\n",
    "            state = jeu.board\n",
    "            current_player = jeu.joueur(state)\n",
    "            b = current_player\n",
    "            # des X et des O au lieu de 1 et 2\n",
    "            if b == 1: b='X'\n",
    "            else: b = 'O'\n",
    "            \n",
    "            if current_player == playerIA: \n",
    "                \n",
    "                qval = model.predict(np.array(state).reshape(1, len(state)), batch_size=batchSize)\n",
    "                print(\"State=\", state)\n",
    "                print(\"Actions:\", jeu.actions)\n",
    "                for i in range(len(qval[0])):\n",
    "                    print(\"     Action:\", i, \"Q-value:\", qval[0][i])\n",
    "                qval_av_action = [-9999]*9\n",
    "                for ac in jeu.actions:\n",
    "                    qval_av_action[ac] = qval[0][ac]\n",
    "                #print(qval_av_action)\n",
    "                action = (np.argmax(qval_av_action))\n",
    "                print(\"Action:\", action)\n",
    "            else:\n",
    "                jeu.myPrint()\n",
    "                print (\"action \", jeu.actions, \" current_player = \", b)\n",
    "                action = int(input(\"Player %s: \" % (current_player)))\n",
    "        \n",
    "            if action == 10:\n",
    "                loop=False\n",
    "                break \n",
    "            \n",
    "            if action in jeu.actions: \n",
    "                jeu.play(state, action)\n",
    "            else: \n",
    "                print (\"----- > Wrong move, try again!\")\n",
    "                \n",
    "            if jeu.asWin():\n",
    "                if current_player == playerIA:\n",
    "                    print(\"------------------------------\")\n",
    "                    print(\"--------->  AI WIN <----------\")\n",
    "                    print(\"------------------------------\")\n",
    "                else:\n",
    "                    print(\"------------------------------\")\n",
    "                    print(\"----------> YOU WIN <---------\")\n",
    "                    print(\"------------------------------\")\n",
    "             \n",
    "            if jeu.nb_move == 9 and jeu.end_game == 0:\n",
    "                print(\"------------------------------\")\n",
    "                print(\"---> No winner, No looser <---\")\n",
    "                print(\"------------------------------\")\n",
    "            \n",
    "            #clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, our agent will be very poor. The network will converge only after a long time, we are here only with CPU and a lot of optimisation could be done to enhance and speed up the deep network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAlgo(model, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "287px",
    "left": "1594px",
    "top": "91.5667px",
    "width": "236px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
